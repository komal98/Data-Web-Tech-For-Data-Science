{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9985b2-c1cf-4db1-a2e0-c8a77f844bcb",
   "metadata": {},
   "source": [
    "<h1>STA 220 ASSIGNMENT 2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4770c3b-263e-4e31-b4ac-115f788c11b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from requests_cache import CachedSession\n",
    "import concurrent.futures\n",
    "import re\n",
    "from queue import Queue\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from lxml import html as lx\n",
    "import itertools\n",
    "from scipy.sparse import lil_matrix, csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6efa9ea-c275-4334-bfc8-83c842ade706",
   "metadata": {},
   "source": [
    "(a) Use the special AllPages page and understand its logic to retrieve the url of all articles in the sinhalese wikipedia. Make sure to skip redirections.\n",
    "\n",
    "How many articles are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d7a27d-234b-4620-8cb0-7d122e24a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://si.wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30fd8f1-93f5-45b4-a9ee-3b41569436af",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = CachedSession()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b522520-840e-437c-a43a-759edfc70189",
   "metadata": {},
   "source": [
    "Method to grab all the articles in a single page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ebd146-a5f1-4e64-a634-2f6a9085e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_articles_in_a_page(url):\n",
    "    response = session.get(url, allow_redirects=False)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the 'ul' element with the class 'mw-allpages-chunk'\n",
    "    unordered_list_articles = soup.find('ul', class_='mw-allpages-chunk')\n",
    "\n",
    "    # Count the number of 'a' elements (links) in the 'ul' element\n",
    "    hyper_links = unordered_list_articles .find_all('a')\n",
    "    article_urls = set()\n",
    "    for link in hyper_links:\n",
    "        href = link.get('href')\n",
    "        article_url = base_url + href\n",
    "        # Check if the link is a redirection link or a link to a special page, image, or external website\n",
    "        if href and href.startswith('/wiki/') and not ':' in href and 'mw-redirect' not in link.get('class', ''):\n",
    "            # This is a non-redirection link to a regular article\n",
    "            article_urls.add(article_url)\n",
    "    return article_urls"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9901a6a0-3cd6-48f0-a561-3396736f0fe7",
   "metadata": {},
   "source": [
    "Parsing through all the pages using the anchor 'Next Page' or 'මීළඟ පිටුව' in sinhalese and adding the articles links in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "368f42c0-b4c1-4928-82f7-a2cc073e06f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_articles(url):\n",
    "    all_article_urls = set()\n",
    "    while url:\n",
    "        article_urls = count_articles_in_a_page(url)\n",
    "        all_article_urls.update(article_urls)\n",
    "        response = session.get(url, allow_redirects=False)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the 'next page' link\n",
    "        next_page_div = soup.find('div', class_='mw-allpages-nav')\n",
    "        next_page_link = next_page_div.find('a', string=lambda value: value and \"මීළඟ පිටුව\" in value)\n",
    "        if next_page_link:\n",
    "            url = base_url + next_page_link.get('href')\n",
    "        else:\n",
    "            url = None\n",
    "    return all_article_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c98e8-ac39-4af4-adb3-a888d7249f53",
   "metadata": {},
   "source": [
    "Calculating the number of articles in Sinhala Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d016716-fe28-40d1-94ad-e1ea6ba1056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Sinhala Wikipedia contains 24186 articles.\n"
     ]
    }
   ],
   "source": [
    "all_page_url = 'https://si.wikipedia.org/wiki/%E0%B7%80%E0%B7%92%E0%B7%81%E0%B7%9A%E0%B7%82:%E0%B7%83%E0%B7%92%E0%B6%BA%E0%B7%85%E0%B7%94_%E0%B6%B4%E0%B7%92%E0%B6%A7%E0%B7%94'\n",
    "all_article_urls = fetch_all_articles(all_page_url)\n",
    "print(f'The Sinhala Wikipedia contains {len(all_article_urls)} articles.')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13b89ce4-da9a-4380-91e4-80a0449902ca",
   "metadata": {},
   "source": [
    "(b, i) Scan all articles in the sinhalese wikipedia and retrieve all links to other articles. Avoid links to special pages, images or the ones that point to another website. Only count the proper article for links that point to a specific section. Use regular expressions to manage these cases. (ii) Make sure to match redirections to their correct destiation article. To this end, find how wikipedia treats redirections and retrieve the true article. (iii) Use threading to request all articles and obtain all links to other articles.\n",
    "\n",
    "How many links to other articles are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bbbce83-2c7f-4457-9dd6-88338cd37774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url_content(url):\n",
    "    response = session.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return lx.fromstring(response.content)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_links_from_content(content, article_url, all_article_urls):\n",
    "    other_links = set()\n",
    "    body_content_div = content.xpath('//div[@id=\"bodyContent\"]')[0]\n",
    "    a_tags = body_content_div.xpath('.//a[@href]')\n",
    "    href_pattern = re.compile(r'^/wiki/[^:]*$')\n",
    "\n",
    "    for a_tag in a_tags:\n",
    "        href = a_tag.get('href')\n",
    "        if href_pattern.match(href) and not href.endswith('.jpg'):\n",
    "            full_url = urljoin(article_url, href)\n",
    "            if full_url in all_article_urls:\n",
    "                other_links.add(full_url)\n",
    "            elif 'mw-redirect' in a_tag.get('class', ''):\n",
    "                redirect_content = fetch_url_content(full_url)\n",
    "                if redirect_content is not None:\n",
    "                    redirected_link = redirect_content.xpath('//span[@class=\"mw-redirectedfrom\"]/a/@href')\n",
    "                    if redirected_link:\n",
    "                        new_url = urljoin(full_url, redirected_link[0])\n",
    "                        if new_url in all_article_urls:\n",
    "                            other_links.add(new_url)\n",
    "    return other_links\n",
    "\n",
    "def fetch_article_and_extract_links(article_url, all_article_urls):\n",
    "    try:\n",
    "        content = fetch_url_content(article_url)\n",
    "        if content is not None:\n",
    "            other_links = extract_links_from_content(content, article_url, all_article_urls)\n",
    "            return article_url, other_links\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {article_url}: {e}\")\n",
    "        return article_url, set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92207888-6060-4f5b-b996-e2b72fa667e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of other articles: 268642\n"
     ]
    }
   ],
   "source": [
    "def fetch_all_articles_and_extract_links(all_article_urls):\n",
    "    dict_links_other_articles = {}\n",
    "    num_threads = 6\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        future_to_url = {executor.submit(fetch_article_and_extract_links, url, all_article_urls): url for url in all_article_urls}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                _, other_links = future.result()\n",
    "                if other_links:\n",
    "                    dict_links_other_articles[url] = other_links\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "    return dict_links_other_articles\n",
    "\n",
    "dict_links_other_articles = fetch_all_articles_and_extract_links(all_article_urls)\n",
    "\n",
    "total_articles = sum(len(links) for links in dict_links_other_articles.values())\n",
    "print(\"Total number of other articles:\", total_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc78741-b49f-4a69-8679-b4ae4f5aa3f1",
   "metadata": {},
   "source": [
    "(c) Compute the transition matrix (see here and here for step-by-step instructions). Make sure to tread dangling nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5806c700-480c-41ca-9125-3cc1c7a60328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transition_matrix(adjacency_csr):\n",
    "    # Compute row sums\n",
    "    row_sums = np.array(adjacency_csr.sum(axis=1)).flatten()\n",
    "    \n",
    "    # Compute inverse row sums\n",
    "    inverse_row_sums = np.divide(1, row_sums, where=row_sums!=0)\n",
    "    \n",
    "    # Compute row stochastic matrix\n",
    "    row_stochastic_matrix = csr_matrix(np.diag(inverse_row_sums)) @ adjacency_csr\n",
    "\n",
    "    # Handle dangling nodes\n",
    "    dangling_nodes = np.where(row_sums == 0)[0]\n",
    "    num_nodes = adjacency_csr.shape[0]\n",
    "    dangling_matrix = np.full(num_nodes, 1 / num_nodes)\n",
    "    row_stochastic_matrix = row_stochastic_matrix.tolil()\n",
    "    row_stochastic_matrix[dangling_nodes, :] = dangling_matrix\n",
    "    row_stochastic_matrix = row_stochastic_matrix.tocsr()\n",
    "\n",
    "    return row_stochastic_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b45a80ea-8e25-4058-860f-35707401b130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t4.134623335814107e-05\n",
      "  (0, 1)\t4.134623335814107e-05\n",
      "  (0, 2)\t4.134623335814107e-05\n",
      "  (0, 3)\t4.134623335814107e-05\n",
      "  (0, 4)\t4.134623335814107e-05\n",
      "  (0, 5)\t4.134623335814107e-05\n",
      "  (0, 6)\t4.134623335814107e-05\n",
      "  (0, 7)\t4.134623335814107e-05\n",
      "  (0, 8)\t4.134623335814107e-05\n",
      "  (0, 9)\t4.134623335814107e-05\n",
      "  (0, 10)\t4.134623335814107e-05\n",
      "  (0, 11)\t4.134623335814107e-05\n",
      "  (0, 12)\t4.134623335814107e-05\n",
      "  (0, 13)\t4.134623335814107e-05\n",
      "  (0, 14)\t4.134623335814107e-05\n",
      "  (0, 15)\t4.134623335814107e-05\n",
      "  (0, 16)\t4.134623335814107e-05\n",
      "  (0, 17)\t4.134623335814107e-05\n",
      "  (0, 18)\t4.134623335814107e-05\n",
      "  (0, 19)\t4.134623335814107e-05\n",
      "  (0, 20)\t4.134623335814107e-05\n",
      "  (0, 21)\t4.134623335814107e-05\n",
      "  (0, 22)\t4.134623335814107e-05\n",
      "  (0, 23)\t4.134623335814107e-05\n",
      "  (0, 24)\t4.134623335814107e-05\n",
      "  :\t:\n",
      "  (24185, 24161)\t4.134623335814107e-05\n",
      "  (24185, 24162)\t4.134623335814107e-05\n",
      "  (24185, 24163)\t4.134623335814107e-05\n",
      "  (24185, 24164)\t4.134623335814107e-05\n",
      "  (24185, 24165)\t4.134623335814107e-05\n",
      "  (24185, 24166)\t4.134623335814107e-05\n",
      "  (24185, 24167)\t4.134623335814107e-05\n",
      "  (24185, 24168)\t4.134623335814107e-05\n",
      "  (24185, 24169)\t4.134623335814107e-05\n",
      "  (24185, 24170)\t4.134623335814107e-05\n",
      "  (24185, 24171)\t4.134623335814107e-05\n",
      "  (24185, 24172)\t4.134623335814107e-05\n",
      "  (24185, 24173)\t4.134623335814107e-05\n",
      "  (24185, 24174)\t4.134623335814107e-05\n",
      "  (24185, 24175)\t4.134623335814107e-05\n",
      "  (24185, 24176)\t4.134623335814107e-05\n",
      "  (24185, 24177)\t4.134623335814107e-05\n",
      "  (24185, 24178)\t4.134623335814107e-05\n",
      "  (24185, 24179)\t4.134623335814107e-05\n",
      "  (24185, 24180)\t4.134623335814107e-05\n",
      "  (24185, 24181)\t4.134623335814107e-05\n",
      "  (24185, 24182)\t4.134623335814107e-05\n",
      "  (24185, 24183)\t4.134623335814107e-05\n",
      "  (24185, 24184)\t4.134623335814107e-05\n",
      "  (24185, 24185)\t4.134623335814107e-05\n"
     ]
    }
   ],
   "source": [
    "all_article_urls = list(set(all_article_urls))\n",
    "\n",
    "# Creating a mapping from URLs to indices\n",
    "url_to_index = {url: index for index, url in enumerate(all_article_urls)}\n",
    "\n",
    "# Initializing an empty adjacency matrix\n",
    "adjacency_matrix = lil_matrix((len(all_article_urls), len(all_article_urls)), dtype=int)\n",
    "\n",
    "# Filling in the adjacency matrix\n",
    "for article_url, linked_articles in dict_links_other_articles.items():\n",
    "    from_index = url_to_index.get(article_url)\n",
    "    for linked_article in linked_articles:\n",
    "        to_index = url_to_index.get(linked_article)\n",
    "        if from_index is not None and to_index is not None:\n",
    "            adjacency_matrix[from_index, to_index] = 1\n",
    "\n",
    "# Converting to CSR format for efficient operations\n",
    "adjacency_csr = adjacency_matrix.tocsr()\n",
    "\n",
    "# Computing transition matrix\n",
    "transition_matrix = compute_transition_matrix(adjacency_csr)\n",
    "\n",
    "print(transition_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e17acd-2ffe-473d-aaef-a1d5bd8699ac",
   "metadata": {},
   "source": [
    "(d, i) Set the damping factor to 0.85 and compute the PageRank for each article, using fourty iterations and starting with a vector with equal entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad15e7e2-659a-42e5-89ae-50edae41b12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.13462334e-05]\n",
      " [4.13462334e-05]\n",
      " [4.13462334e-05]\n",
      " ...\n",
      " [4.13462334e-05]\n",
      " [4.13462334e-05]\n",
      " [4.13462334e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Setting the damping factor\n",
    "damping_factor = 0.85\n",
    "\n",
    "# Initializing the PageRank vector with equal entries\n",
    "pagerank_vector = np.full((len(all_article_urls), 1), 1 / len(all_article_urls))\n",
    "\n",
    "# Computing the PageRank for each article using 40 iterations\n",
    "for _ in range(40):\n",
    "    pagerank_vector = (1 - damping_factor) / len(all_article_urls) + damping_factor * transition_matrix @ pagerank_vector\n",
    "\n",
    "print(pagerank_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b237e-2c84-43b5-ae2c-440dccc7cf4d",
   "metadata": {},
   "source": [
    "(d,ii) Obtain the top ten articles in terms of PageRank, and, retrieving the articles again, find the correponding English article, if available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5507ed-4bac-49d9-8e46-137126e68223",
   "metadata": {},
   "source": [
    "Return the corresponding English article titles of the top ten articles from the Sinhalese wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb4cb01e-eccc-4b68-93dd-3598986ea4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 URL\n",
      "0  https://si.wikipedia.org/wiki/%E0%B7%83%E0%B7%...\n",
      "1  https://si.wikipedia.org/wiki/%E0%B6%A2%E0%B7%...\n",
      "2  https://si.wikipedia.org/wiki/%E0%B6%89%E0%B6%...\n",
      "3  https://si.wikipedia.org/wiki/%E0%B6%B6%E0%B7%...\n",
      "4  https://si.wikipedia.org/wiki/%E0%B6%B4%E0%B7%...\n",
      "5  https://si.wikipedia.org/wiki/%E0%B6%B8%E0%B7%...\n",
      "6  https://si.wikipedia.org/wiki/%E0%B6%A0%E0%B7%...\n",
      "7  https://si.wikipedia.org/wiki/%E0%B6%B4%E0%B6%...\n",
      "8  https://si.wikipedia.org/wiki/%E0%B7%81%E0%B7%...\n",
      "9  https://si.wikipedia.org/wiki/%E0%B6%86%E0%B6%...\n"
     ]
    }
   ],
   "source": [
    "# Getting the indices of the top 10 articles\n",
    "top_10_indices = np.argsort(pagerank_vector, axis=0)[-10:]\n",
    "\n",
    "# Getting the URLs of the top 10 articles\n",
    "top_10_urls = [all_article_urls[i] for i in top_10_indices.flatten()]\n",
    "\n",
    "df = pd.DataFrame(top_10_urls, columns=['URL'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c94e777-46cd-4811-929a-d5781b148cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://si.wikipedia.org/wiki/%E0%B6%A2%E0%B7%94%E0%B6%BD%E0%B7%92%E0%B6%BA%E0%B7%8F_%E0%B6%B8%E0%B7%8F%E0%B6%9C%E0%B6%BB%E0%B6%A7%E0%B7%8A_%E0%B6%9A%E0%B7%90%E0%B6%B8%E0%B6%BB%E0%B6%B1%E0%B7%8A\n",
      "English version: https://en.wikipedia.org/wiki/Julia_Margaret_Cameron\n",
      "\n",
      "URL: https://si.wikipedia.org/wiki/%E0%B6%B4%E0%B7%93%E0%B6%B1%E0%B7%92%E0%B7%82%E0%B7%92%E0%B6%BA%E0%B7%8F%E0%B7%80\n",
      "English version: https://en.wikipedia.org/wiki/Phoenicia\n",
      "\n",
      "URL: https://si.wikipedia.org/wiki/%E0%B6%B8%E0%B7%8F%E0%B6%BB%E0%B7%8A%E0%B6%9C%E0%B6%9C%E0%B6%AD_%E0%B6%B4%E0%B7%94%E0%B7%80%E0%B6%AD%E0%B7%8A%E0%B6%B4%E0%B6%AD%E0%B7%8A\n",
      "English version: https://en.wikipedia.org/wiki/Online_newspaper\n",
      "\n",
      "URL: https://si.wikipedia.org/wiki/%E0%B6%86%E0%B6%BD%E0%B7%9D%E0%B6%9A%E0%B6%BA%E0%B7%9A_%E0%B7%80%E0%B7%9A%E0%B6%9C%E0%B6%BA\n",
      "English version: https://en.wikipedia.org/wiki/Speed_of_light\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initializing a dictionary to store the English versions of the articles\n",
    "english_versions = {}\n",
    "\n",
    "# Fetching the English version for each article\n",
    "for url in top_10_urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    english_link = soup.find('a', {'hreflang': 'en'})\n",
    "    if english_link is not None:\n",
    "        english_versions[url] = english_link['href']\n",
    "\n",
    "# Printing the English versions\n",
    "for url, english_version in english_versions.items():\n",
    "    print(f\"URL: {url}\")\n",
    "    print(f\"English version: {english_version}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
